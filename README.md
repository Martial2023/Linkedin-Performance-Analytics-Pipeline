# Analyse des Performances des Posts LinkedIn

## üìú Description du Projet

Ce projet vise √† analyser les performances des publications sur LinkedIn pour identifier les facteurs cl√©s qui maximisent l'engagement et la viralit√© des posts. En exploitant des donn√©es scrap√©es directement depuis LinkedIn, le pipeline extrait, transforme et analyse les informations (th√®mes, hashtags, moments de publication, longueur des posts) pour calculer des KPI exploitables. Ces KPI permettent de r√©pondre √† des questions strat√©giques : sur quels th√®mes publier, quels hashtags utiliser, quelle longueur adopter, et quand publier pour optimiser l‚Äôimpact.

Le pipeline est automatis√© avec **Apache Airflow** pour un rafra√Æchissement quotidien des donn√©es, int√©grant des √©tapes de scraping, validation, stockage (PostgreSQL et MongoDB), et analyse avanc√©e (segmentation DBSCAN). Les r√©sultats sont visualis√©s via un tableau de bord interactif Next.js, disponible dans un d√©p√¥t s√©par√© :  
[**LinkedIn Performance Dashboard Repository**](https://github.com/your-username/linkedin-performance-dashboard)

---

## üìä KPI Calcul√©s

Les KPI suivants ont √©t√© calcul√©s pour guider la strat√©gie de publication sur LinkedIn :
- **Th√®mes viraux** : Fr√©quence des th√®mes comme `WorkplaceCulture`, `DigitalTransformation`, `Leadership`, `IA`, et `Tutoriel`.
- **Hashtags strat√©giques** : Popularit√© des hashtags comme `#digitaltransformation`, `#leadership`, `#ai`, et `#workplaceculture`.
- **Moments optimaux** : Engagement moyen par jour et p√©riode (par exemple, mercredi soir : 530.5, vendredi fin de journ√©e : 359.7).
- **Longueur des posts** : Longueur moyenne des posts √† fort engagement (~215 mots) vs faible engagement (~165 mots).
- **Segmentation des posts** : Clusters identifi√©s via DBSCAN pour r√©v√©ler des patterns sp√©cifiques (par exemple, posts narratifs ou tutoriels techniques).

Ces KPI permettent d‚Äôoptimiser le contenu, le timing, et le ciblage des publications pour maximiser l‚Äôengagement et la visibilit√©.

---

## üõ†Ô∏è Architecture du Pipeline

Le pipeline est con√ßu pour collecter, traiter, et analyser les donn√©es LinkedIn de mani√®re automatis√©e. Voici les √©tapes principales :

1. **Scraping des donn√©es** : Collecte des posts, engagements, th√®mes, et hashtags depuis LinkedIn avec Python, Selenium, et BeautifulSoup.
2. **Premi√®re transformation** : Nettoyage et structuration des donn√©es, sauvegard√©es temporairement en fichiers `.parquet`.
3. **Validation des donn√©es** : V√©rification de l‚Äôint√©grit√© (valeurs manquantes, formats, doublons) pour garantir la qualit√©.
4. **Stockage dans PostgreSQL** : Chargement des donn√©es valid√©es dans une base PostgreSQL pour un stockage structur√©.
5. **Extraction vers Parquet** : Exportation des donn√©es PostgreSQL vers des fichiers `.parquet` pour les traitements ult√©rieurs.
6. **Deuxi√®me transformation et calcul des KPI** : Calcul des KPI avec Polars, visualisations interm√©diaires avec Plotly, et stockage des r√©sultats dans MongoDB.
7. **Automatisation** : Orchestration quotidienne du pipeline avec Apache Airflow pour scraper et mettre √† jour les donn√©es.
8. **Segmentation DBSCAN** : Analyse des clusters pour identifier des patterns non d√©tect√©s (par exemple, types de posts engageants).

---

## üñºÔ∏è Architecture Finale

![Architecture Finale du Pipeline](pipeline.png)

---

## üöÄ Mise en Place du Pipeline et Automatisation

### Pr√©requis

- **Python 3.10+** install√©.
- **PostgreSQL** : Une instance configur√©e (locale ou cloud, par exemple, Neon).
- **MongoDB** : Un cluster configur√© (par exemple, MongoDB Atlas) avec une base `linkedin_kpi_db`.
- **Apache Airflow** : Install√© localement ou sur un serveur.
- **D√©pendances Python** : List√©es dans `requirements.txt`.
- **Navigateur compatible avec Selenium** : Chrome ou Firefox avec le driver correspondant (par exemple, ChromeDriver).

### √âtape 1 : Cloner le Projet

Clonez le d√©p√¥t contenant les scripts du pipeline :
```bash
git clone https://github.com/Martial2023/Linkedin-Performance-Analytics-Pipeline
cd linkedin-performance-pipeline
```

### √âtape 2 : Installer les D√©pendances

Installez les d√©pendances Python √† partir de `requirements.txt` :
```bash
pip install -r requirements.txt
```

Contenu de `requirements.txt` :
```
scikit-learn
pymongo
pandas
sqlalchemy
psycopg2-binary
polars
plotly
selenium
beautifulsoup4
requests
webdriver-manager
apache-airflow
```

### √âtape 3 : Configurer les Variables d‚ÄôEnvironnement

Cr√©ez un fichier `.env` dans le r√©pertoire racine ainsi que dans le dossier 'airflow_dags' et ajoutez les variables suivantes :
```
# Connexion MongoDB
MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/linkedin_kpi_db?retryWrites=true&w=majority

LINKEDIN_USER_NAME=""
LINKEDIN_PWD=""
# Connexion PostgreSQL
DATABASE_URL="postgresql://..."
```

### √âtape 4 : Configurer Apache Airflow

1. **Initialiser Airflow** :
   Configurez Airflow et initialisez la base de donn√©es :
   ```bash
   export AIRFLOW_HOME=~/airflow
   airflow db init
   ```

2. **Cr√©er un Utilisateur Airflow** :
   ```bash
   airflow users create \
     --username admin \
     --firstname Admin \
     --lastname User \
     --role Admin \
     --email admin@example.com
   ```

3. **Copier le DAG** :
   Placez le fichier DAG (par exemple, `linkedin_pipeline.py`) dans le dossier `~/airflow/dags/` :
   ```bash
   cp dags/linkedin_pipeline.py ~/airflow/dags/
   ```

### √âtape 5 : Lancer Airflow

1. **D√©marrer le Serveur Web Airflow** :
   ```bash
   airflow webserver --port 8080
   ```

2. **D√©marrer le Scheduler Airflow** (dans un autre terminal) :
   ```bash
   airflow scheduler
   ```

3. **Acc√©der √† l‚ÄôInterface Airflow** :
   - Ouvrez votre navigateur et allez √† `http://localhost:8080`.
   - Connectez-vous avec les identifiants cr√©√©s (par exemple, `admin`).
   - Activez le DAG `linkedin_pipeline` en cliquant sur le bouton "Toggle".

Le pipeline s‚Äôex√©cutera automatiquement tous les jours pour scraper de nouvelles publications LinkedIn et mettre √† jour les KPI. Vous pouvez √©galement le d√©clencher manuellement via l‚Äôinterface Airflow.

---

## üìà Dashboard

Pour visualiser les KPI via un tableau de bord interactif Next.js, consultez le d√©p√¥t d√©di√© :  
[**LinkedIn Performance Dashboard Repository**](https://github.com/your-username/linkedin-performance-dashboard)

---

## üìù Conclusion

Ce projet fournit une solution robuste pour analyser les performances des publications LinkedIn, en identifiant les th√®mes, hashtags, moments de publication, et longueurs de posts optimaux pour maximiser l‚Äôengagement. Le pipeline automatis√© avec Airflow garantit des donn√©es √† jour, tandis que la segmentation DBSCAN r√©v√®le des insights avanc√©s. Les KPI calcul√©s permettent aux cr√©ateurs de contenu de prendre des d√©cisions √©clair√©es pour optimiser leur strat√©gie sur LinkedIn.

---

## ü§ù Contributions

Les contributions sont les bienvenues ! Si vous souhaitez ajouter de nouveaux KPI, am√©liorer le pipeline, ou optimiser le scraping, ouvrez une issue ou soumettez une pull request sur le d√©p√¥t GitHub.

---

## üìú Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.